{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cb4a42-33a3-4a87-a31e-6eba13a95aa7",
   "metadata": {},
   "source": [
    "# Database Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cda9b-edb1-4c51-a1b0-531c1f83ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import csv\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f2789-b5a7-4eba-bea4-34a34ee3c3cc",
   "metadata": {},
   "source": [
    "## Google Scholar Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09df2e6-f187-43d9-addb-fffb60acdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_scholar():\n",
    "    \n",
    "    # URL to search for scientists. The keyword is 'physic', which will hopefully \n",
    "    # return most of the physics scientists. The Indian search is enforced through\n",
    "    # the domains .ac.in and .res.in. Currently searches only through first page,\n",
    "    # needs to be extended to all pages\n",
    "    \n",
    "    url = \"https://scholar.google.co.in/citations?hl=en&view_op=search_authors&mauthors=physic+%2B+.ac.in+%7C+.res.in&btnG=\"\n",
    "       \n",
    "    page = requests.get(url)\n",
    "    soup = bsoup(page.content, features='lxml')\n",
    "    results = {}\n",
    "    \n",
    "    count = 1\n",
    "    while soup != []:\n",
    "        print (\"G Scholar: Page\", count)\n",
    "        count += 1\n",
    "        \n",
    "        tags = soup.findAll('h3', attrs={'class': \"gs_ai_name\"})\n",
    "        if tags == []:\n",
    "            print (\"There's a problem.\")\n",
    "        for tag in tags:\n",
    "\n",
    "            # obtain name, affiliation, interests and homepage (if exists)\n",
    "            name = tag.text\n",
    "            link = \"https://scholar.google.com\"+tag.next['href']\n",
    "            author_soup = bsoup(requests.get(link).content, features='lxml')\n",
    "            affil_tag = author_soup.find('div', attrs={'class':\"gsc_prf_il\"})\n",
    "            affil = affil_tag.text\n",
    "            try:\n",
    "                homepage = author_soup.find('a', text = \"Homepage\")['href']\n",
    "            except:\n",
    "                homepage = \"\"\n",
    "            interests = [child.text for child in author_soup.find('div', attrs={'class':\"gsc_prf_il\", 'id':\"gsc_prf_int\"}).findChildren()]\n",
    "\n",
    "            data = [name, affil, homepage, ', '.join(interests)]\n",
    "\n",
    "            # append data for this scientist to the dictionary\n",
    "            for interest in interests:\n",
    "\n",
    "                # sanitise interest by changing space to\n",
    "                # _ and converting all to lower case\n",
    "                interest_sanitised = interest.replace(' ', '_').lower()\n",
    "\n",
    "                # create key if does not exist\n",
    "                if interest_sanitised not in results:\n",
    "                    results[interest_sanitised] = [data]\n",
    "                else:\n",
    "                    results[interest_sanitised].append(data)\n",
    "            \n",
    "        try:\n",
    "            next_btn = soup.find('button', attrs={'aria-label': \"Next\", 'class': \"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\"})\n",
    "            url = 'https://scholar.google.co.in' + str(next_btn['onclick']).replace('window.location=\\'', '')[:-1]\n",
    "            index = url.find(\"\\\\x\")\n",
    "            while index >= 0:\n",
    "                hexa = url[index+2:index+4]\n",
    "                actual = bytearray.fromhex(hexa).decode()\n",
    "                url = url.replace(\"\\\\x\"+hexa, actual)\n",
    "                index = url.find(\"\\\\x\")\n",
    "            page = requests.get(url)\n",
    "            soup = bsoup(page.content, features='lxml')\n",
    "        except:\n",
    "            soup = []\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383d30d-f6f4-43da-8cbd-33ec9b83bbef",
   "metadata": {},
   "source": [
    "## IISc Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddecac3-edba-486c-a458-152708713123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iisc():\n",
    "    results = {}\n",
    "    url_aap = \"http://www.physics.iisc.ac.in/~jap/people-fac.html\"\n",
    "    fname = \"file.html\"\n",
    "    os.system(\"wget \" + url_aap + \" -O \" + fname + \" >/dev/null 2>&1\")\n",
    "    soup = bsoup(open(fname, 'r'))\n",
    "    for tag in soup.findAll('div', attrs={'class': \"about-veno\"}):\n",
    "        name = tag.findNext('h3').text\n",
    "        desig = tag.findNext('h4').text\n",
    "        interests = str(list(tag.findNext('h5', text='Research Interests:').next_siblings)[0]).replace('\\n', '').strip().split('.')\n",
    "        homepage = tag.findNext('span', text='Web:').findNext('a')['href']\n",
    "        data = [name, \"IISc\", desig, homepage, ', '.join(interests)]\n",
    "\n",
    "        # append data for this scientist to the dictionary\n",
    "        for interest in interests:\n",
    "\n",
    "            # sanitise interest by changing space to\n",
    "            # _ and converting all to lower case\n",
    "            interest_sanitised = interest.strip().replace(' ', '_').lower()\n",
    "\n",
    "            # create key if does not exist\n",
    "            if interest_sanitised not in results:\n",
    "                results[interest_sanitised] = [data]\n",
    "            else:\n",
    "                results[interest_sanitised].append(data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44840f-022e-47b8-844a-9f904c3e089b",
   "metadata": {},
   "source": [
    "# Dump results into files\n",
    "\n",
    "**WARNING: Existing files are overwritten at present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e13e82-6801-4f51-9c25-fc2cc3ebfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headings of each CSV file\n",
    "headings = ['Name', 'Affiliation', 'Homepage', 'Interests']\n",
    "\n",
    "# path of database folder, inside which all the\n",
    "# csv files will live. Folder is automatically\n",
    "# created if it does not exist.\n",
    "folder = \"./physics_database/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# obtain dictionaries from all existing retrievers\n",
    "results_arr = []\n",
    "results_arr.append(iisc())\n",
    "\n",
    "# write results of each dictionary into each csv file\n",
    "for results in results_arr:\n",
    "    # print (results)\n",
    "    for key in results:\n",
    "        print (key)\n",
    "        print (1)\n",
    "        # file is opened in append mode 'a', this ensures\n",
    "        # that existing files are not overwritten\n",
    "        csv_w = csv.writer(open(folder+key+\".csv\", 'w'), delimiter='\\t')\n",
    "        data = results[key]\n",
    "        csv_w.writerow(headings)\n",
    "        csv_w.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa963e9-9884-43ca-ad6d-a2267f143591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
